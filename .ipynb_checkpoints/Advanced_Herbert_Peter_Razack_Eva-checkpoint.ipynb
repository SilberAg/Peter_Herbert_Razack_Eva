{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZIRIBAGWA PETER MSC. BIOINFORMATICS\n",
    "### AGASI HERBERT   MSC. BIOINFORMATICS\n",
    "### RAZACK WASSWA   MSC. BIOINFORMATICS\n",
    "### EVA AKURUT      MSC. BIOINFORMATICS\n",
    "### GROUP  ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 - LOADING THE NECCESARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n",
    "#pip install eli5\n",
    "#pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "#statistics\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "#Missing Info\n",
    "import missingno as msno\n",
    "\n",
    "#Dimentionality Reduction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "from sklearn import cluster\n",
    "import sklearn\n",
    "\n",
    "#scikit learn libraries\n",
    "#Lineaer Models\n",
    "from sklearn.linear_model import BayesianRidge,RidgeCV,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_squared_log_error,make_scorer\n",
    "from sklearn.preprocessing import normalize,MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Lineaer Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Ensemble\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "\n",
    "##\n",
    "#eli5\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "##\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "#Kaggle \n",
    "#import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    " #   for filename in filenames:\n",
    "  #      print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "#let's remove the annoying warnings from our cells.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 - IMPORTING THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "## Loading the data\n",
    "Train = pd.read_csv(\"data/train.csv\")\n",
    "Test = pd.read_csv(\"data/test.csv\")\n",
    "Ids =Test[\"Id\"]\n",
    "Train.index = Train[\"Id\"]\n",
    "Test.index =Test[\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 - EXPLANATORY DATA ANALYSIS\n",
    "\n",
    "### 3.1 - Display the first 10 rows of the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Checking the shape of the train dataset\n",
    "- This out puts the number of rows and columns within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Display the first 10 rows of the test dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Descriptive statistics on the Targert column Sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.SalePrice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 A Histogram  and Line graph representing the distribution of the Sale Price\n",
    "#### Looking at the distribution of the data \n",
    "- There are very few records above the 500,000 mark.\n",
    "- The prices in this range are exorbitant have no special feature hence dropping them is good idea \n",
    "- The prices are also skewed to the left. A test to confirm and if the dropping the above columns will fix the skew\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train.SalePrice.plot(kind='box')\n",
    "plt.hist(Train.SalePrice)\n",
    "#plt.density(Train.SalePrice)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "sns.distplot(Train['SalePrice'], fit=norm, color=\"green\")\n",
    "plt.title('An histogram showing the distribution of the SalePrice across the train dataset', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train['SalePrice'].plot(fontsize=30,figsize = (30,10), color ='red')\n",
    "plt.title('Line Graph showing the distribution of the SalePrice across the train set', fontsize=30)\n",
    "plt.ylabel('sale price',fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for the skewness of the SalePrice\n",
    "- The skew is 1.8828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train[\"SalePrice\"].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the the rows that have values greater than 520000\n",
    "- There are 8 rows with values above the 520000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train[Train[\"SalePrice\"]>520000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping records greater than 510000\n",
    "#Train = Train['SalePrice'] - Train[Train['SalePrice']>600000]\n",
    "#Salesindex = Train[Train['SalePrice']>600000].index\n",
    "#Train.drop(Salesindex,inplace=True)\n",
    "Train.drop(Train[Train['SalePrice']>520000].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the skew after dropping the columns above 520000\n",
    "- The skew in 1.20800 there is reduction in the skewness however not good enough\n",
    "- The dimensions of the Train dataset are (1452, 81)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train[\"SalePrice\"].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for the datatypes for the Train Dataset\n",
    "- The data has 43 categorical features and 38 Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.dtypes[:30], Train.dtypes[30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of the categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering out columns that have categorical data to loo\n",
    "arr=[]\n",
    "for x,y in enumerate(Train.dtypes):\n",
    "    if(y==\"object\"):\n",
    "        arr.append(x)\n",
    "x=Train.columns[arr]\n",
    "print(x)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at t preview of the data these columns have a very suspicous\n",
    "1. MasVnrArea\n",
    "2. BsmtFinSF2\n",
    "3. 2ndFlrSF\n",
    "4. WoodDeckSF\n",
    "5. OpenPorchSF\n",
    "6. EnclosedPorch\n",
    "7. 3SsnPorch\n",
    "8. ScreenPorch\n",
    "9. PoolArea\n",
    "10. PoolQC\n",
    "11. Fence\n",
    "12. MiscFeature\n",
    "13. MiscVal\n",
    "14. MoSold\n",
    "15. LowQualFinSF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A plot showing the missing data matrix of the train dataset\n",
    "- This shows the missing data patterns with in the database\n",
    "- The sparkline on the right gives a summary of the general shape of the data completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(Train)\n",
    "plt.title('A PLOT SHOWING THE MISSING DATA MATRIX', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A heatmap showing the nullity of the train dataset\n",
    "- The range of this nullity correlation is from -1 to 1 (-1 ≤ R ≤ 1); the features with no missing value are excluded in the heatmap. If the nullity correlation is very close to zero (-0.05 < R < 0.05), no value will be displayed.\n",
    "- A perfect positive nullity correlation (R=1) indicates when the first feature and the second feature both have corresponding missing values while a perfect negative nullity correlation (R=-1) means that one of the features is missing and the second is not missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(Train,figsize=(15,8))\n",
    "plt.title('A HEATMAP SHOWING A CORRELATION OF MISSING DATA BETWEEN ATTRIBUTES IN THE TRAIN DATASET', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualising the percentage of  missing data per column using a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_clone = Train.copy() # this makes a clone of the train dataset and names it 'train_clone'\n",
    "\n",
    "### Now i use the train_clone dataset to compute the percentage of unknown values in each column\n",
    "columns_with_NaNs = Train_clone.isnull().sum() #  first assign all the output into this object\n",
    "columns_with_NaNs = columns_with_NaNs.drop(columns_with_NaNs[columns_with_NaNs==0].index) # filter out only those greater than 0. this because zero means no NaNs and those greater than zero indicate the presence of NaNs\n",
    "columns_with_NaNs = columns_with_NaNs.sort_values(ascending=False) # sort them in descending order\n",
    "columns_with_NaNs =  columns_with_NaNs / len(Train_clone)*100\n",
    "\n",
    "### Plot the percentages on a histogram\n",
    "f, ax = plt.subplots(figsize=(15,8))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=columns_with_NaNs.index, y=columns_with_NaNs)\n",
    "plt.xlabel('Attributes', fontsize=20)\n",
    "plt.ylabel('Percentage of null values', fontsize=20)\n",
    "plt.title('Percentage of null values per attribute', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping columns with high number of nulls \n",
    "- Alley           1369\n",
    "- PoolQC           1453\n",
    "- Fence            1179\n",
    "- MiscFeature      1406\n",
    "- LotFrontage      259\n",
    "- FireplaceQu       690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train =Train.drop(axis=1,columns=[\"FireplaceQu\",\"LotFrontage\",\"MiscFeature\",\"Fence\",\"PoolQC\",\"Alley\"])\n",
    "\n",
    "\n",
    "Tran =Train.dropna(axis=0, how=\"any\",subset=[\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\n",
    "                                             \"BsmtFinType2\",\"GarageType\",\"GarageYrBlt\",\"GarageFinish\",\n",
    "                                             \"GarageQual\",\"GarageCond\",\"MasVnrType\",\"MasVnrArea\",\"Electrical\"\n",
    "                                            ],inplace=True)\n",
    "\n",
    "Test =Test.drop(axis=1,columns=[\"FireplaceQu\",\"LotFrontage\",\"MiscFeature\",\"Fence\",\"PoolQC\",\"Alley\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming there are no nulls\n",
    "Train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Missing values int Test dataset\n",
    "Test = Test.fillna(Test.mean())\n",
    "Test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise correlation between numerical features and the saleprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select out only numerical values\n",
    "Train_numerical = Train.select_dtypes(include=[np.number])\n",
    "\n",
    "fig, axs = plt.subplots(13,3, figsize=(16, 30), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=.2, right=0.95)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for ind, col in enumerate(Train_numerical.columns):\n",
    "    if col != 'SalePrice':\n",
    "        sns.regplot(Train_numerical[col], Train_numerical['SalePrice'], ax = axs[ind])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Visualizing the distribution of the categorical features in relation to the saleprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('chained',None)\n",
    "\n",
    "# select out only columns having categorical data i.e, exclude those with numbers as shown in the command below\n",
    "Train_categorical  = Train.select_dtypes(exclude=[np.number])\n",
    "\n",
    "#add the SalePrice column\n",
    "Train_categorical['SalePrice'] = Train['SalePrice']\n",
    "\n",
    "fig, axs = plt.subplots(14,3, figsize=(16, 30), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=.2, right=0.95)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for ind, col in enumerate(Train_categorical.columns):\n",
    "    if col != 'SalePrice':\n",
    "        sns.barplot(Train_categorical[col], Train_categorical['SalePrice'], ax = axs[ind])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the categorical variables\n",
    "# converting type of columns to 'category'\n",
    "#bridge_df['Bridge_Types'] = bridge_df['Bridge_Types'].astype('category')\n",
    "# Assigning numerical values and storing in another column\n",
    "#bridge_df['Bridge_Types_Cat'] = bridge_df['Bridge_Types'].cat.codes\n",
    "#bridge_df\n",
    "Catcols = Train.select_dtypes(include=\"O\")\n",
    "\n",
    "for cols in Catcols:\n",
    "    # converting type of columns to 'category'\n",
    "    Train[cols] = Train[cols].astype('category')\n",
    "    # Assigning numerical values \n",
    "    Train[cols] = Train[cols].cat.codes\n",
    "Train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodoing the Test data as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testcols = Test.select_dtypes(include=\"O\")\n",
    "for cols in Testcols:\n",
    "    # converting type of columns to 'category'\n",
    "    Test[cols] =Test[cols].astype('category')\n",
    "    # Assigning numerical values \n",
    "    Test[cols] = Test[cols].cat.codes\n",
    "Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.isna().sum()[:30],Test.isna().sum()[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Train.drop(\"SalePrice\",axis=1) \n",
    "y=Train.SalePrice\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb1=XGBRegressor()\n",
    "xgb1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "ETR = ExtraTreesRegressor()\n",
    "ETR.fit(X_train,y_train)\n",
    "\n",
    "RANDF= RandomForestRegressor()\n",
    "RANDF.fit(X_train,y_train)\n",
    "\n",
    "perm  = PermutationImportance(xgb1).fit(X_test, y_test)\n",
    "perm1 = PermutationImportance(ETR).fit(X_test, y_test)\n",
    "perm2 = PermutationImportance(RANDF).fit(X_test, y_test)\n",
    "#eli5.show_weights(perm)\n",
    "dir(eli5)\n",
    "#eli5.permutation_importance.get_score_importances(perm)\n",
    "eli5.explain_weights_df(perm)[:15],eli5.explain_weights_df(perm1)[:15],eli5.explain_weights_df(perm2)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection And Description\n",
    "1. 44 GrLivArea\n",
    "    - Above grade (ground) living area square feet\n",
    "2. 15 OverallQual\n",
    "    - Rates the overall material and finish of the house\n",
    "3. 36 TotalBsmtSF\n",
    "    - Total square feet of basement area\n",
    "4. 17 YearBuilt\n",
    "    - Original construction date\n",
    "5. 32 BsmtFinSF1\n",
    "    - Type 1 finished square feet\n",
    "6. 18 YearRemodAdd\n",
    "    - Remodel date (same as construction date if no remodeling or additions)\n",
    "7. 3 LotArea\n",
    "    - Lot size in square feet\n",
    "8. 51 KitchenQual\n",
    "    - Kitchen quality\n",
    "9. 16 OverallCond\n",
    "    - Rates the overall condition of the house\n",
    "10. 58 GarageCars\n",
    "    - Size of garage in car capacity\n",
    "11.  28 ExterCond\n",
    "    - Evaluates the present condition of the material on the exterior\n",
    "12.  1 MSSubClass\n",
    "    - Identifies the type of dwelling involved in the sale.\n",
    "13. 2 MSZoning\n",
    "    - Identifies the general zoning classification of the sale.\n",
    "14. 57 FireplaceQu\n",
    "    - Fireplace quality\n",
    "15. 41 1stFlrSF\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train1 =Train[[\"GrLivArea\",\"OverallQual\", \"TotalBsmtSF\",\"BsmtFinSF1\",\"LotArea\",\"KitchenQual\",\"YearBuilt\",\"YearRemodAdd\",\"GarageCars\",\"1stFlrSF\",\"BsmtFullBath\",\"CentralAir\",\"HouseStyle\",\"SalePrice\"]]\n",
    "Train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation of the selected Features with SalePrice\n",
    "- As seen from the graph below a number of features are correlated with the SalePrice hence willposivitely impact model\n",
    "- Two of the features are however negatice correlated i.e. KitchenQual and MSZoning which mean an inverse relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train1.corr()['SalePrice'].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since we only what features that are highly correlated we will drop the \n",
    "#OverallCond and MSSUblass from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.iloc[1:5,41].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the features basing on Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential columns for data analysis\n",
    "#Train[\"YearBuilt\"] = pd.to_datetime(Train[\"YearBuilt\"],format=\"%y\")\n",
    "#Train[\"YearRemodAdd\"] = pd.to_datetime(Train[\"YearRemodAdd\"],format=\"%y\")\n",
    "#Train.dtypes[:30]\n",
    "\n",
    "#Train =Train[[\"GrLivArea\",\"OverallQual\",\"TotalBsmtSF\",\"YearBuilt\",\"BsmtFinSF1\",\"YearRemodAdd\",\"LotArea\",\"KitchenQual\",\n",
    " #     \"OverallCond\",\"GarageCars\",\"ExterCond\",\"MSSubClass\",\"MSZoning\",\"FireplaceQu\",\"SalePrice\"]]\n",
    "#Train =Train.SalePrice\n",
    "\n",
    "#Train =Train[[\"GrLivArea\",\"OverallQual\", \"TotalBsmtSF\",\"YearBuilt\",\"BsmtFinSF1\",\"YearRemodAdd\",\"LotArea\",\n",
    "#\"KitchenQual\",\"GarageCars\",\"ExterCond\",\"MSZoning\",\"1stFlrSF\",\"SalePrice\"]]\n",
    "\n",
    "#Train.drop(Train[[\"ExterCond\",\"KitchenQual\",\"MSZoning\",\"LotArea\",\"BsmtFinSF1\"]],axis=0)\n",
    "#,\"KitchenQual\"\n",
    "#,\"KitchenQual\"\n",
    "#,\"YearBuilt\"\n",
    "#,\"YearBuilt\"\n",
    "#,\"YearRemodAdd\"\n",
    "#,\"YearBuilt\"\n",
    "#,\"YearBuilt\"\n",
    "Train =Train[[\"GrLivArea\",\"OverallQual\", \"TotalBsmtSF\",\"BsmtFinSF1\",\"LotArea\",\"KitchenQual\",\"YearBuilt\",\"YearRemodAdd\",\"GarageCars\",\"1stFlrSF\",\"BsmtFullBath\",\"CentralAir\",\"HouseStyle\",\"SalePrice\"]]\n",
    "\n",
    "Test = Test[[\"GrLivArea\",\"OverallQual\", \"TotalBsmtSF\",\"BsmtFinSF1\",\"LotArea\",\"KitchenQual\",\"YearBuilt\",\"YearRemodAdd\",\"GarageCars\",\"1stFlrSF\",\"BsmtFullBath\",\"CentralAir\",\"HouseStyle\"]]\n",
    "Test = Test.fillna(Test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting and Reshaping\n",
    "array = Train.values\n",
    "# separate array into input and output components\n",
    "#X = array[:,0:8]\n",
    "#Y = array[:,8]\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# Recaling the My Test data\n",
    "scalert = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledTest =scalert.fit_transform(Test)\n",
    "seed =42\n",
    "# splitting the faeture array and label array keeping 80% for the trainnig sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(rescaledX,y,test_size=0.20,shuffle=True,random_state=seed)\n",
    "\n",
    "# normalize: Scale input vectors individually to unit norm (vector length).\n",
    "X_train = normalize(X_train)\n",
    "X_test=normalize(X_test)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This segment of code is where the principle component analysis is done.\n",
    "\n",
    "n_comp = 4\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_train = tsvd.fit_transform(X_train)\n",
    "tsvd_test = tsvd.transform(X_test)\n",
    "\n",
    "# PCA Train\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_train = pca.fit_transform(X_train)\n",
    "pca2_test = pca.transform(X_test)\n",
    "\n",
    "#PCA Test\n",
    "pca = PCA(n_components = n_comp, random_state=420)\n",
    "#pca2_test = pca.fit_transform(Test)\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_train = ica.fit_transform(X_train)\n",
    "ica2_test = ica.transform(X_test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(X_train)\n",
    "grp_results_test = grp.transform(X_test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(X_train)\n",
    "srp_results_test = srp.transform(X_test)\n",
    "\n",
    "# NMF\n",
    "nmf = NMF(n_components=n_comp, init='nndsvdar', random_state=420)\n",
    "nmf_results_train = nmf.fit_transform(X_train)\n",
    "nmf_results_test = nmf.transform(X_test)\n",
    "\n",
    "## FAG\n",
    "fag = cluster.FeatureAgglomeration(n_clusters=n_comp, linkage='ward')\n",
    "fag_results_train = fag.fit_transform(X_train)\n",
    "fag_results_test = fag.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Metrics\n",
    "#### Mean Absolute Error\n",
    "- The mean absolute Error is the average of the absolute differences between predictions and actual values.\n",
    "- It gives an idea of how wrong the predictions are. \n",
    "- A value of 0 indicates no error or perfect predications\n",
    "- Like logloass the metric is inverted by cross_val_score() function\n",
    "- #neg_mean_absolute_error\n",
    "\n",
    "\n",
    "#### Mean Squared Error\n",
    "- This is simillar to the Mean absolute error in that it provides a gross idea of the magnitude of error.\n",
    "- Taking thesquare root of the mean squared error converts the units back into the original units of the output variable and can be meaningful for description and presentation.\n",
    "- #neg_mean_squred_error\n",
    "\n",
    "\n",
    "#### R2 metric\n",
    "- The R2 metric provides an indication of the goodness of fit of a set of predications to teh actual values.\n",
    "- In statistical it is called the coefficient of determination. \n",
    "- The value is between 0 and 1\n",
    "- #r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPOT CHECKING THE ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPOT CHECKING THE ALGORITHMS\n",
    "# Spot-Check Algorithms\n",
    "\n",
    "models = []\n",
    "#models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "#models.append(('SVM', SVC()))\n",
    "#models.append(('NN',MLPRegressor()))#//Poor performance\n",
    "models.append(('SG',SGDRegressor()))\n",
    "models.append(('REG',RidgeCV()))\n",
    "models.append(('BRR',BayesianRidge()))\n",
    "models.append(('XGBOOST',XGBRegressor()))\n",
    "models.append((\"RANDF\",RandomForestRegressor()))\n",
    "models.append((\"BREGR\",BaggingRegressor()))\n",
    "#models.append((\"STACKr\",StackingRegressor())) #Requires Estimators\n",
    "models.append((\"HGBR\",HistGradientBoostingRegressor()))\n",
    "models.append((\"ETR\",ExtraTreesRegressor()))\n",
    "models.append((\"ABR\",AdaBoostRegressor()))\n",
    "## Root Mean Squared Log Error\n",
    "#rmsle = math.sqrt(mean_squared_log_error())\n",
    "#rmsle = make_scorer(math.sqrt(mean_squared_log_error()))\n",
    "# evaluate each model in turn\n",
    "\n",
    "def rmsle(y,ypred):\n",
    "    y =np.array(y)\n",
    "    ypred=np.array(ypred)\n",
    "    assert len(y) == len(ypred)\n",
    "    # X_used: numpy.ndarray  {n_samples, n_features_used}\n",
    "    terms_to_sum= [(math.log(ypred[i]+1) - math.log(y[i]+1)) **2.0 for i,pred in enumerate(ypred)]\n",
    "    return (sum(terms_to_sum)* (1.0/len(y))) ** 0.5\n",
    "\n",
    "#scorer = {'main': 'r2',\n",
    " #         'custom_rmsle': make_scorer(rmsle,needs_xvals = True)}\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "seed =42\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed,shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, \n",
    "    scoring='r2')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))\n",
    "#print(msg)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sorted(sklearn.metrics.SCORERS.keys())\n",
    "help(type(LogisticRegression))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting Classifier\n",
    "##old-algorithms\n",
    "#REG = RidgeCV()\n",
    "#REG.fit(X_train,y_train)\n",
    "#KNN = KNeighborsRegressor()\n",
    "#KNN.fit(X_train,y_train)\n",
    "\n",
    "#XGB = XGBRegressor()\n",
    "#XGB.fit(X_train,y_train)\n",
    "\n",
    "#Updated Algorithms\n",
    "ETR = ExtraTreesRegressor()\n",
    "ETR.fit(X_train,y_train)\n",
    "\n",
    "HGBR = HistGradientBoostingRegressor()\n",
    "HGBR.fit(X_train,y_train)\n",
    "\n",
    "RANDF = RandomForestRegressor()\n",
    "RANDF.fit(X_train,y_train)\n",
    "\n",
    "XGBOOST= XGBRegressor()\n",
    "XGBOOST.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "ensemble=VotingRegressor(estimators=[('ETR', ETR),('HGBR',HGBR),(\"RANDF\",RANDF),(\"XGBOOST\",XGBOOST)])\n",
    "y_pred =ensemble.fit(X_train,y_train).predict(X_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(y_pred,y_test))\n",
    "print(RMSLE)\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(r2_score(y_pred,y_test ))\n",
    "\n",
    "Test = normalize(rescaledTest)\n",
    "knn_pred =ensemble.predict(Test)\n",
    "fypred =pd.DataFrame(Ids)\n",
    "fypred[\"SalePrice\"] = pd.DataFrame(knn_pred)\n",
    "#frpred[\"Id\"] = Test[\"Id\"]\n",
    "fypred.to_csv(\"Voting_Regressor.csv\", header =[\"Id\",\"SalePrice\"], index =False,sep=\",\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "These  are some of the parameters we will be tunning \n",
    "\n",
    "#### n_estimators integer, optional (default=10) \n",
    "- The number of trees in the forest.\n",
    "\n",
    "#### max_depth integer or None, optional (default=None)\n",
    "- The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "#### min_samples_leaf int, float, optional (default=1)\n",
    "- The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "#### min_samples_split int, float, optional (default=2)\n",
    "- The minimum number of samples required to split an internal node\n",
    "\n",
    "#### bootstrapboolean, optional (default=True)\n",
    "- Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.\n",
    "\n",
    "#### max_features int, float, string or None, optional (default=”auto”)\n",
    "The number of features to consider when looking for the best split:\n",
    "\n",
    "- If int, then consider max_features features at each split.\n",
    "- If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "- If “auto”, then max_features=n_features.\n",
    "- If “sqrt”, then max_features=sqrt(n_features).\n",
    "- If “log2”, then max_features=log2(n_features).\n",
    "- If None, then max_features=n_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters ={'bootstrap': [True, False],\n",
    "             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': [1, 2, 4],\n",
    "             'min_samples_split': [2, 5, 10],\n",
    "             'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "#parameters = {'n_estimators':range(1,10), 'max_depth':range(1,5)}\n",
    "RANDF = RandomForestRegressor()\n",
    "model = GridSearchCV(RANDF, parameters)\n",
    "model.fit(X_train, y_train)\n",
    "#print(model.best_estimator_)\n",
    "#print(model.cv_results_)\n",
    "\n",
    "\n",
    "RANDF = model.best_estimator_\n",
    "RANDF.fit(X_train,y_train)\n",
    "RANDF_pred = RANDF.predict(X_test)\n",
    "MSE = mean_squared_error(RANDF_pred,y_test)\n",
    "r2 = r2_score(RANDF_pred,y_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(RANDF_pred, y_test))\n",
    "print(MSE)\n",
    "print(r2)\n",
    "print(RMSLE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':range(1,15), 'max_depth':range(1,10)}\n",
    "ETR = ExtraTreesRegressor()\n",
    "model = GridSearchCV(RANDF, parameters)\n",
    "model.fit(X_train, y_train)\n",
    "#print(model.best_estimator_)\n",
    "\n",
    "ETR = model.best_estimator_\n",
    "ETR.fit(X_train,y_train)\n",
    "ETR_pred = ETR.predict(X_test)\n",
    "MSE = mean_squared_error(ETR_pred,y_test)\n",
    "r2 = r2_score(ETR_pred,y_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(ETR_pred, y_test))\n",
    "print(MSE)\n",
    "print(r2)\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate':[0.1,0.2,0.3], 'max_depth':range(1,10),\"random_state\":[42]}\n",
    "HGBR = HistGradientBoostingRegressor()\n",
    "model = GridSearchCV(HGBR, parameters)\n",
    "model.fit(X_train, y_train)\n",
    "HGBR = model.best_estimator_\n",
    "HGBR.fit(X_train,y_train)\n",
    "HGBR_pred = HGBR.predict(X_test)\n",
    "MSE = mean_squared_error(HGBR_pred,y_test)\n",
    "r2 = r2_score(HGBR_pred,y_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(HGBR_pred, y_test))\n",
    "print(MSE)\n",
    "print(r2)\n",
    "print(RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searsch Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Searsch Ridge Regressor\n",
    "\n",
    "parameters = [{'alpha':[0.001,0.1,1,100,1000,10000]}]\n",
    "RR = Ridge()\n",
    "Grid1 = GridSearchCV(RR,parameters,cv=4)\n",
    "Grid1.fit(X_train,y_train)\n",
    "print(Grid1.best_estimator_)\n",
    "scores = Grid1.cv_results_\n",
    "print(scores['mean_test_score'])\n",
    "Grid_pred = Grid1.predict(X_test)\n",
    "MSE = mean_squared_error(Grid_pred,y_test)\n",
    "r2 = r2_score(Grid_pred,y_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(Grid_pred, y_test))\n",
    "rmsle1 =rmsle(Grid_pred, y_test)\n",
    "print(MSE)\n",
    "print(r2)\n",
    "print(RMSLE)\n",
    "print(rmsle1)\n",
    "\n",
    "alpha_vals =  [0.001, 0.1, 1, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbours Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "# Number of neighbors to use by default for kneighbors queries.\n",
    "k_range = list(range(1, 16))\n",
    "param_grid = dict(n_neighbors=k_range,p=[1,2])\n",
    "#param_grid={'n_neighbors': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],'p':[1,2]}\n",
    "knn = KNeighborsRegressor()\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='r2')\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_estimator_)\n",
    "scores = grid.cv_results_\n",
    "\n",
    "print(scores['mean_test_score'])\n",
    "\n",
    "\n",
    "knn=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=9, p=1,\n",
    "                    weights='uniform')\n",
    "knn.fit(X_train,y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "MSE = mean_squared_error(knn_pred,y_test)\n",
    "r2 = r2_score(knn_pred,y_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(knn_pred,y_test))\n",
    "\n",
    "print(MSE)\n",
    "print(r2)\n",
    "print(RMSLE)\n",
    "#Test =Test.fillna(Test.mean(),inplace=True)\n",
    "#Test.reshape(-1, 1)\n",
    "Test = normalize(rescaledTest)\n",
    "knn_pred =knn.predict(Test)\n",
    "\n",
    "fypred =pd.DataFrame(Ids)\n",
    "fypred[\"SalePrice\"] = pd.DataFrame(knn_pred)\n",
    "fypred.to_csv(\"Knn_rescaled2.csv\", header =[\"Id\",\"SalePrice\"], index = False,sep=\",\" )\n",
    "#Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch XGBOOST\n",
    "#\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "X_test = normalize(X_test)\n",
    "parameters ={\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"eta\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]\n",
    " }\n",
    "model = XGBRegressor()\n",
    "Grid2 = GridSearchCV(model,parameters,cv=10)\n",
    "Grid2.fit(X_train,y_train)\n",
    "y_pred = Grid2.predict(X_test)\n",
    "#y_pred = pd.DataFrame(y_pred)\n",
    "MSE = mean_squared_error(y_pred,y_test)\n",
    "r2 = r2_score(y_pred,y_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(y_pred,y_test))\n",
    "print(MSE)\n",
    "print(r2)\n",
    "print(RMSLE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "params={'eta':0.3,'max_depth':3,'objective':'multi:softprob', 'num_class':3}\n",
    "steps = 40\n",
    "#{\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " #\"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " #\"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " #\"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " #\"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n",
    "#eta=0.3,max_depth=4\n",
    "model = Grid2.best_estimator_\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "MSE = mean_squared_error(y_pred,y_test)\n",
    "r2 = r2_score(y_pred,y_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(y_pred,y_test))\n",
    "print(MSE)\n",
    "print(r2)\n",
    "print(RMSLE)\n",
    "\n",
    "Test = normalize(rescaledTest)\n",
    "grid_preds =model.predict(rescaledTest)\n",
    "fypred =pd.DataFrame(Ids)\n",
    "fypred[\"SalePrice\"] = pd.DataFrame(grid_preds)\n",
    "#frpred[\"Id\"] = Test[\"Id\"]\n",
    "fypred.to_csv(\"XGBOOST_rescaled.csv\", header =[\"Id\",\"SalePrice\"], index = False,sep=\",\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Regressor with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting Classifier\n",
    "#CART = DecisionTreeRegressor()\n",
    "#CART.fit(X_train,y_train)\n",
    "\n",
    "#KNN = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "#                    metric_params=None, n_jobs=None, n_neighbors=11, p=1,\n",
    "#                    weights='uniform')\n",
    "#KNN.fit(X_train,y_train)\n",
    "\n",
    "HGBR = HistGradientBoostingRegressor()\n",
    "HGBR.fit(X_train,y_train)\n",
    "\n",
    "ETR = ExtraTreesRegressor()\n",
    "ETR.fit(X_train,y_train)\n",
    "\n",
    "XGB = XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "                     colsample_bynode=1, colsample_bytree=1, eta=0.1, gamma=0,\n",
    "                     gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
    "                     learning_rate=0.100000001, max_delta_step=0, max_depth=4,\n",
    "                     min_child_weight=7,  monotone_constraints=None,\n",
    "                     n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
    "                     objective='reg:squarederror', random_state=42, reg_alpha=0,\n",
    "                     reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
    "                     validate_parameters=False, verbosity=None)\n",
    "XGB.fit(X_train,y_train)\n",
    "\n",
    "ensemble=VotingRegressor(estimators=[('HGBR', HGBR),('ETR',ETR),(\"XGBOOST\",XGB)])\n",
    "y_pred =ensemble.fit(X_train,y_train).predict(X_test)\n",
    "RMSLE = math.sqrt(mean_squared_log_error(y_pred,y_test))\n",
    "print(RMSLE)\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(r2_score(y_pred,y_test ))\n",
    "\n",
    "Test = normalize(rescaledTest)\n",
    "VC_test =ensemble.predict(Test)\n",
    "fypred =pd.DataFrame(Ids)\n",
    "fypred[\"SalePrice\"] = pd.DataFrame(VC_test)\n",
    "#frpred[\"Id\"] = Test[\"Id\"]\n",
    "fypred.to_csv(\"Voting_Regressor_HP.csv\", header =[\"Id\",\"SalePrice\"], index =False,sep=\",\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
